{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fc80feb9",
   "metadata": {},
   "source": [
    "### Import dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "36cecd9b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-22T06:02:37.929938Z",
     "start_time": "2024-02-22T06:02:37.919156Z"
    }
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "\n",
    "# # Mount Google Drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2059c61b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-22T06:02:46.157440Z",
     "start_time": "2024-02-22T06:02:37.932813Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 617
    },
    "id": "2059c61b",
    "outputId": "576afb16-4157-4337-f4db-d04264361874"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: not using Google CoLab\n",
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "# import category_encoders as ce\n",
    "# import copy\n",
    "# import polars as pl\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pylab as plt\n",
    "from matplotlib import font_manager, rc\n",
    "import matplotlib\n",
    "import seaborn as sns\n",
    "# import plotly.express as px\n",
    "# %matplotlib inline\n",
    "# matplotlib.rcParams['font.family'] = 'Malgun Gothic' # ÌïúÍ∏Ä Ìå®Ïπò\n",
    "# Preprocessing & Feature Engineering\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler, LabelEncoder, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "from sklearn.feature_selection import SelectPercentile\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Hyperparameter Optimization\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "# Modeling\n",
    "# from sklearn.dummy import DummyClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "# from sklearn.neighbors import KNeighborsClassifier\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "# from sklearn.neural_network import MLPClassifier\n",
    "# from sklearn.tree import DecisionTreeClassifier\n",
    "# from sklearn.ensemble import ExtraTreesClassifier\n",
    "# from sklearn.ensemble import GradientBoostingClassifier\n",
    "from xgboost import XGBClassifier, XGBRegressor, XGBRFRegressor\n",
    "from lightgbm import LGBMClassifier, LGBMRegressor\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import VotingClassifier, VotingRegressor\n",
    "from sklearn.ensemble import StackingClassifier, StackingRegressor\n",
    "# from sklearn.base import ClassifierMixin\n",
    "\n",
    "# CatBoost\n",
    "# from catboost import CatBoostRegressor\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn.init as init\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import Parameter\n",
    "from torch import Tensor\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "# Evaluation\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import log_loss,mean_squared_error\n",
    "import sklearn\n",
    "\n",
    "# Utility\n",
    "import os\n",
    "import time\n",
    "import datetime # ‚ö†Ô∏è2019ÎÖÑ 12Ïõî30ÏùºÍ≥º 31ÏùºÏùò week of yearÍ∞Ä 1Ïù∏ Ïò§Î•òÍ∞Ä ÏûàÏùå\n",
    "import random\n",
    "import warnings; warnings.filterwarnings(\"ignore\")\n",
    "from IPython.display import Image\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import platform\n",
    "from itertools import combinations,product\n",
    "from scipy.stats.mstats import gmean\n",
    "import holidays\n",
    "\n",
    "# from bayes_opt import BayesianOptimization\n",
    "# from num2words import num2words\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "from statsmodels.stats.outliers_influence import OLSInfluence\n",
    "\n",
    "pd.set_option('display.max_row',None)\n",
    "pd.set_option('display.max_column',None)\n",
    "\n",
    "### Setting universal random_state\n",
    "np.random.seed(142)\n",
    "random.seed(142)\n",
    "sklearn.utils.check_random_state(142)\n",
    "torch.manual_seed(142)\n",
    "torch.backends.cudnn.deterministic=True\n",
    "torch.backends.cudnn.benchmark=False\n",
    "\n",
    "try:\n",
    "    import google.colab\n",
    "    COLAB = True\n",
    "    print(\"Note: using Google CoLab\")\n",
    "except:\n",
    "    print(\"Note: not using Google CoLab\")\n",
    "    COLAB = False\n",
    "\n",
    "# Make use of a GPU or MPS (Apple) if one is available.  (see module 3.2)\n",
    "has_mps = torch.backends.mps.is_built()\n",
    "device = \"mps\" if has_mps else \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "zVBHDA4xub-S",
   "metadata": {
    "id": "zVBHDA4xub-S"
   },
   "source": [
    "### Merge dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "jPRldaW5c653",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-22T06:02:46.188465Z",
     "start_time": "2024-02-22T06:02:46.160435Z"
    },
    "id": "jPRldaW5c653"
   },
   "outputs": [],
   "source": [
    "class PrepareData():\n",
    "    def __init__(self,stock_fp,news_fp):\n",
    "        self.stock_filepath = stock_fp\n",
    "        self.news_filepath = news_fp\n",
    "        self.topic_classes = ['CEO', 'CFO', 'Layoffs', 'Political', 'PressRelease', 'Undefined',\n",
    "       'cramer', 'earnings', 'gold', 'manda', 'paylimitwall', 'paywall',\n",
    "       'product', 'recession', 'tanalysis'] # undefinedÏùò classÍ∞Ä 5\n",
    "\n",
    "    def load_data(self):\n",
    "        stock = pd.read_csv(self.stock_filepath,index_col=0)\n",
    "        news = pd.read_csv(self.news_filepath,index_col=0)\n",
    "        ### parse date manually\n",
    "        stock['Date'] = pd.to_datetime(stock['Date'])\n",
    "        news['date'] = pd.to_datetime(news['date'])\n",
    "        return stock, news\n",
    "\n",
    "    def merging(self, stock, news):\n",
    "        ### fill na value of PINS column\n",
    "#         stock['PINS'] = stock['PINS'].fillna(stock['PINS'].iloc[75])\n",
    "\n",
    "        ### drop 'news_id' column\n",
    "#         news = news.drop(columns=['news_id'])\n",
    "\n",
    "        ### add date range from 18.01.02 to 18.12.31\n",
    "        temp_range = pd.DataFrame(dict(zip(stock.columns,[pd.date_range(start='2018-01-02',end='2018-12-31'),\n",
    "                                0,0,0,0, # 4\n",
    "                                0,0,0,0,\n",
    "                                0,0,0,0,\n",
    "                                0,0,0,0, # 16\n",
    "                                0,0,0,0,\n",
    "                                0,0,0,0,\n",
    "                                0,0,0,0, # 28\n",
    "                                0,0,0,0,\n",
    "                                0,0,0,0, # 36\n",
    "                                0 # 37\n",
    "                            ])))\n",
    "        stock_inc = pd.concat([temp_range,stock],axis=0)\n",
    "\n",
    "        ### merge stock_inc and news\n",
    "        # left = stock_inc\n",
    "        # on = date\n",
    "        # how = left\n",
    "        # rename 'date' to 'Date' of news df\n",
    "        news = news.rename(columns={'date':'Date'})\n",
    "        merged = pd.merge(left=stock_inc,right=news,on='Date',how='left')\n",
    "\n",
    "        ### Cut before 2018-02-13\n",
    "        merged = merged[42:].reset_index(drop=True)\n",
    "\n",
    "        # fill na with latest non-null values\n",
    "        columns_to_fill = ['source_name', 'topics', 'rank_score',\n",
    "                        'sentiment_Negative','sentiment_Neutral',\n",
    "                        'sentiment_Positive', 'type_Article', 'type_Video']\n",
    "        merged_fillna = merged.copy()\n",
    "        for column in columns_to_fill:\n",
    "            merged_fillna[column].fillna(method='ffill',inplace=True)\n",
    "\n",
    "        ### add moving average to sentiments\n",
    "        ma_nums = [5,60,120]\n",
    "        def mode_window(window):\n",
    "            return window.mode().iloc[0] if not len(window.mode())==0 else None\n",
    "        for num in ma_nums:\n",
    "            merged_fillna[f'{num}MA_sent_Neg']=merged_fillna['sentiment_Negative'].rolling(\n",
    "            window=num).mean()\n",
    "            merged_fillna[f'{num}MA_sent_Neu']=merged_fillna['sentiment_Neutral'].rolling(\n",
    "            window=num).mean()\n",
    "            merged_fillna[f'{num}MA_sent_Pos']=merged_fillna['sentiment_Positive'].rolling(\n",
    "            window=num).mean()\n",
    "        ### add moving mode to sentiments\n",
    "        for num in ma_nums:\n",
    "            merged_fillna[f'{num}MM_sent_Neg']=merged_fillna['sentiment_Negative'].rolling(\n",
    "            window=num).apply(mode_window)\n",
    "            merged_fillna[f'{num}MM_sent_Neu']=merged_fillna['sentiment_Neutral'].rolling(\n",
    "            window=num).apply(mode_window)\n",
    "            merged_fillna[f'{num}MM_sent_Pos']=merged_fillna['sentiment_Positive'].rolling(\n",
    "            window=num).apply(mode_window)\n",
    "        ### adding moving mode to topics\n",
    "        for num in ma_nums:\n",
    "            merged_fillna[f'{num}MM_topics']=merged_fillna['topics'].rolling(\n",
    "            window=num).apply(mode_window)\n",
    "\n",
    "        ### drop before 2019-01-02\n",
    "        total_df = merged_fillna.iloc[322:]\n",
    "        total_df = total_df.reset_index(drop=True)\n",
    "\n",
    "        ### drop unnecessaray columns\n",
    "        drop_cols = ['source_name','topics','rank_score',\n",
    "                    'sentiment_Negative','sentiment_Neutral',\n",
    "                    'sentiment_Positive','type_Article','type_Video']\n",
    "        total_df = total_df.drop(columns=drop_cols)\n",
    "\n",
    "        return total_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cXT6Ts-1dPIr",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-22T06:02:48.619683Z",
     "start_time": "2024-02-22T06:02:46.190464Z"
    },
    "id": "cXT6Ts-1dPIr"
   },
   "outputs": [],
   "source": [
    "stock_filepath = '../../data/stock_price/netflix_60.csv' # Í∞ÅÏûê ÌååÏùº Í≤ΩÎ°ú ÏÑ§Ï†ï\n",
    "news_filepath = '../../data/scraping/news_processed_filtered_2.csv'\n",
    "# stock_filepath = './drive/MyDrive/Colab Notebooks/data/bitamin_mini_project/netflix_60.csv'\n",
    "# news_filepath = './drive/MyDrive/Colab Notebooks/data/bitamin_mini_project/news_processed_filtered_2.csv'\n",
    "loader = PrepareData(stock_filepath, news_filepath)\n",
    "stock_df, news_df=loader.load_data() # >> Í∞êÏÑ±Î∂ÑÏÑù ÎØ∏Ìè¨Ìï®ÏúºÎ°ú Î™®Îç∏ ÎèåÎ¶¥ Îïê stock_df Î∞îÎ°ú ÏÇ¨Ïö©ÌïòÎ©¥ Îê®\n",
    "total_df = loader.merging(stock=stock_df, news=news_df) # Ï£ºÏãùÎç∞Ïù¥ÌÑ∞ÏÖãÏóê Í∞êÏÑ±Î∂ÑÏÑù,ÌÜ†ÌîΩ Ìè¨Ìï®ÏãúÌÇ® Ï†ÑÏ≤¥ Îç∞Ïù¥ÌÑ∞ÏÖã\n",
    "testfile = '../../data/test.csv'\n",
    "test_df = pd.read_csv(testfile)\n",
    "\n",
    "stock_df.index = stock_df[\"Date\"]\n",
    "stock_df.drop(columns = \"Date\", inplace = True)\n",
    "stock_df[\"PINS\"].fillna(24.99, inplace = True)\n",
    "total_df.index = total_df[\"Date\"]\n",
    "total_df.drop(columns = \"Date\", inplace = True)\n",
    "total_df[\"PINS\"].fillna(24.99, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a39907b2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-22T06:02:48.634833Z",
     "start_time": "2024-02-22T06:02:48.621686Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# pd.set_option('display.max_row',20)\n",
    "# pd.set_option('display.max_column',8)\n",
    "# total_df[['5MA_sent_Neg','5MA_sent_Neu','5MA_sent_Pos','60MA_sent_Neg',\n",
    "#           '60MA_sent_Neu','60MA_sent_Pos','120MA_sent_Neg','120MA_sent_Neu',\n",
    "#           '120MA_sent_Pos','5MM_sent_Neg','5MM_sent_Neu','5MM_sent_Pos',\n",
    "#           '60MM_sent_Neg','60MM_sent_Neu','60MM_sent_Pos','120MM_sent_Neg',\n",
    "#           '120MM_sent_Neu','120MM_sent_Pos','5MM_topics','60MM_topics',\n",
    "#           '120MM_topics']].iloc[0:115]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "956bfbf4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-22T06:02:48.665614Z",
     "start_time": "2024-02-22T06:02:48.637285Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>data</th>\n",
       "      <th>target</th>\n",
       "      <th>seq_size</th>\n",
       "      <th>pred_size</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>hidden_size</th>\n",
       "      <th>best_val_loss</th>\n",
       "      <th>mean_error_ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   data  target  seq_size  pred_size  batch_size  hidden_size  best_val_loss  \\\n",
       "0     0       0         0          0           0            0              0   \n",
       "\n",
       "   mean_error_ratio  \n",
       "0                 0  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " result_df = pd.DataFrame({\n",
    "    'data':[0],\n",
    "    'target':[0],\n",
    "    'seq_size':[0],\n",
    "    'pred_size':[0],\n",
    "    'batch_size':[0],\n",
    "    'hidden_size':[0],\n",
    "    'best_val_loss':[0],\n",
    "    'mean_error_ratio':[0]\n",
    "})\n",
    "result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d72e804a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef3a74ee",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-02-22T06:02:37.938Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üî∏START - Transformer_total_df_1d_ROC_seq30_batch1_hidden64üî∏\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                           | 0/50 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "best_losses = []\n",
    "mean_error_ratios = []\n",
    "\n",
    "datanames = ['total_df']#,'stock_df']\n",
    "\n",
    "for dataname in datanames:\n",
    "    ### Choose dataset & Hyperparameter setting\n",
    "    if dataname=='total_df':\n",
    "        data = total_df\n",
    "    else:\n",
    "        data = stock_df\n",
    "    MODEL = 'Transformer'                 # 'LSTM' / 'GRU' / 'Transformer'\n",
    "    TARGET = \"1d_ROC\"                      # \"Close\" / \"1d_ROC\"\n",
    "    SEQ_SIZE = 30                         # 30 / 60 / 120\n",
    "    PRED_SIZE = 10\n",
    "    BATCH_SIZE = 1                        # 1 / 4 / 8\n",
    "    HIDDEN_SIZE = 64                      # 64 / 128\n",
    "    EPOCHS = 1000\n",
    "    \n",
    "    filename = f'{MODEL}_{dataname}_{TARGET}_seq{SEQ_SIZE}_batch{BATCH_SIZE}_hidden{HIDDEN_SIZE}'\n",
    "\n",
    "    print(f'üî∏START - {filename}üî∏')\n",
    "    ### Make train datset\n",
    "\n",
    "    def split_xy(dataset, time_steps, y_column):\n",
    "        x, y = list(), list()\n",
    "        for i in range(len(dataset)):\n",
    "            x_end_number = i + time_steps\n",
    "            y_end_number = x_end_number + y_column\n",
    "\n",
    "            if y_end_number > len(dataset):\n",
    "                break\n",
    "            tmp_x = dataset.iloc[i:x_end_number, :]  # Adjusted for Pandas\n",
    "            tmp_y = dataset.iloc[x_end_number:y_end_number, :].loc[:, TARGET]\n",
    "            x.append(tmp_x.values)  # Convert to numpy array\n",
    "            y.append(tmp_y.values)  # Convert to numpy array\n",
    "\n",
    "        return np.array(x), np.array(y)\n",
    "\n",
    "    X, y = split_xy(data, SEQ_SIZE, PRED_SIZE)\n",
    "#     print(X[0,:],\"\\n\", y[0])\n",
    "#     print(\"X size : \", X.shape)\n",
    "#     print(\"y size : \", y.shape)\n",
    "\n",
    "    ### Define X_test\n",
    "\n",
    "    X_test = data.tail(SEQ_SIZE).values.reshape(1, SEQ_SIZE, data.shape[1])\n",
    "#     print(X_test)\n",
    "#     print(\"X_test size : \", X_test.shape)\n",
    "\n",
    "    ### Standardization\n",
    "\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "    X = X.reshape(X.shape[0], X.shape[1] * X.shape[2])\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(X)\n",
    "    X = scaler.transform(X)\n",
    "    X = X.reshape(X.shape[0], SEQ_SIZE, data.shape[1])\n",
    "\n",
    "    X_test = X_test.reshape(X_test.shape[0], X_test.shape[1] * X_test.shape[2])\n",
    "    X_test = scaler.transform(X_test)\n",
    "    X_test = X_test.reshape(X_test.shape[0], SEQ_SIZE, data.shape[1])\n",
    "\n",
    "#     print(\"X size : \", X.shape)\n",
    "#     print(\"X_test size : \", X_test.shape)\n",
    "\n",
    "    ### Split train-validation dataset\n",
    "\n",
    "    # to DataLoader\n",
    "    import torch\n",
    "    from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "    from sklearn.model_selection import train_test_split\n",
    "\n",
    "    for trial in tqdm(list(range(1,51))):\n",
    "        X_train, X_valid, y_train, y_valid = train_test_split(X, y, random_state = 1, test_size = 0.2)\n",
    "\n",
    "        # to tensor\n",
    "        X_train = torch.tensor(X_train.astype(np.float32), dtype = torch.float32)\n",
    "        X_valid = torch.tensor(X_valid.astype(np.float32), dtype = torch.float32)\n",
    "        y_train = torch.tensor(y_train.astype(np.float32), dtype = torch.float32)\n",
    "        y_valid = torch.tensor(y_valid.astype(np.float32), dtype = torch.float32)\n",
    "\n",
    "        # to DataLoader\n",
    "        train_loader = DataLoader(TensorDataset(X_train, y_train), batch_size = BATCH_SIZE, shuffle = True)\n",
    "        val_loader = DataLoader(TensorDataset(X_valid, y_valid), batch_size = BATCH_SIZE, shuffle = False)\n",
    "\n",
    "        ### Initialize Model\n",
    "        try:\n",
    "            for param in model.parameters():\n",
    "                if param.requires_grad:\n",
    "                    if len(param.shape) > 1:\n",
    "                        init.xavier_uniform_(param)\n",
    "                    else:\n",
    "                        init.zeros_(param)\n",
    "        except:\n",
    "            try:\n",
    "                model.reset_parameters()\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "        ### Modeling\n",
    "\n",
    "        import copy\n",
    "        class EarlyStopping:\n",
    "            def __init__(self, patience = 5, min_delta = 0, restore_best_weights = True):\n",
    "                self.patience = patience\n",
    "                self.min_delta = min_delta\n",
    "                self.restore_best_weights = restore_best_weights\n",
    "                self.best_model = None\n",
    "                self.best_loss = None\n",
    "                self.counter = 0\n",
    "                self.status = \"\"\n",
    "\n",
    "            def __call__(self, model, val_loss):\n",
    "                if self.best_loss is None:\n",
    "                    self.best_loss = val_loss\n",
    "                    self.best_model = copy.deepcopy(model.state_dict())\n",
    "                elif self.best_loss - val_loss >= self.min_delta:\n",
    "                    self.best_model = copy.deepcopy(model.state_dict())\n",
    "                    self.best_loss = val_loss\n",
    "                    self.counter = 0\n",
    "                    self.status = f\"Improvement found, counter reset to {self.counter}\"\n",
    "                else:\n",
    "                    self.counter += 1\n",
    "                    self.status = f\"No improvement in the last {self.counter} epochs\"\n",
    "                    if self.counter >= self.patience:\n",
    "                        self.status = f\"Early stopping triggered after {self.counter} epochs.\"\n",
    "                        if self.restore_best_weights:\n",
    "                            model.load_state_dict(self.best_model)\n",
    "                        return True\n",
    "                return False\n",
    "\n",
    "        # Positional Encoding for Transformer\n",
    "        class PositionalEncoding(nn.Module):\n",
    "            def __init__(self, d_model, dropout=0.1, max_len=1000):\n",
    "                super(PositionalEncoding, self).__init__()\n",
    "                self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "                pe = torch.zeros(max_len, d_model)\n",
    "                position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "                div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-np.log(10000.0) / d_model))\n",
    "                pe[:, 0::2] = torch.sin(position * div_term)\n",
    "                pe[:, 1::2] = torch.cos(position * div_term)\n",
    "                pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "                self.register_buffer('pe', pe)\n",
    "\n",
    "            def forward(self, x):\n",
    "                x = x + self.pe[:x.size(0), :]\n",
    "                return self.dropout(x)\n",
    "\n",
    "        # Model definition using Transformer\n",
    "        class TransformerModel(nn.Module):\n",
    "            def __init__(self, input_dim, d_model, nhead=4, num_layers=2, dropout=0.2, output_size=10):\n",
    "                super(TransformerModel, self).__init__()\n",
    "\n",
    "                self.encoder = nn.Linear(input_dim, d_model,bias=True)\n",
    "                self.pos_encoder = PositionalEncoding(d_model, dropout)\n",
    "                encoder_layers = nn.TransformerEncoderLayer(d_model, nhead,bias=True)\n",
    "                self.transformer_encoder = nn.TransformerEncoder(encoder_layers, num_layers)\n",
    "                self.decoder = nn.Linear(d_model, output_size,bias=True)\n",
    "\n",
    "            def forward(self, x):\n",
    "                x = self.encoder(x)\n",
    "                x = self.pos_encoder(x)\n",
    "                x = self.transformer_encoder(x)\n",
    "                x = self.decoder(x[:, -1, :])\n",
    "                return x\n",
    "\n",
    "        class RMSELoss(nn.Module):\n",
    "            def __init__(self):\n",
    "                super(RMSELoss,self).__init__()\n",
    "                self.mse = nn.MSELoss()\n",
    "\n",
    "            def forward(self,yhat,y):\n",
    "                return torch.sqrt(self.mse(yhat,y))\n",
    "\n",
    "        model = TransformerModel(X_train.shape[2],\n",
    "                                d_model = HIDDEN_SIZE,\n",
    "                                output_size = PRED_SIZE).to(device)\n",
    "        criterion = RMSELoss()\n",
    "        optimizer = optim.Adam(model.parameters(), lr = 0.001)\n",
    "        scheduler = ReduceLROnPlateau(optimizer, 'min', factor = 0.1, patience = 20, verbose = True)\n",
    "\n",
    "        ### RUN!!\n",
    "\n",
    "        epoch_counter = 0\n",
    "        patience = 30\n",
    "        best_loss = float('inf')\n",
    "        done = False\n",
    "        es = EarlyStopping(patience=patience)\n",
    "        tr_losses_fp, val_losses_fp = [],[]\n",
    "\n",
    "        while not done and epoch_counter<EPOCHS:\n",
    "            epoch_counter+=1\n",
    "\n",
    "            # train\n",
    "            model.train()\n",
    "            train_losses = []\n",
    "            for batch in train_loader:\n",
    "                x_batch, y_batch = batch\n",
    "                x_batch = x_batch.to(device)\n",
    "                y_batch = y_batch.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                output = model(x_batch)\n",
    "                loss = criterion(output,y_batch)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                train_losses.append(loss.item())\n",
    "            train_loss = np.mean(train_losses)\n",
    "            tr_losses_fp.append(train_loss)\n",
    "\n",
    "            # validation\n",
    "            model.eval()\n",
    "            val_losses = []\n",
    "            with torch.no_grad():\n",
    "                for batch in val_loader:\n",
    "                    x_batch, y_batch = batch\n",
    "                    x_batch = x_batch.to(device)\n",
    "                    y_batch = y_batch.to(device)\n",
    "                    output = model(x_batch)\n",
    "                    loss = criterion(output, y_batch)\n",
    "                    val_losses.append(loss.item())\n",
    "            val_loss = np.mean(val_losses)\n",
    "            val_losses_fp.append(val_loss)\n",
    "            scheduler.step(val_loss)\n",
    "\n",
    "            if es(model, val_loss):\n",
    "                done = True\n",
    "\n",
    "            if val_loss < best_loss:\n",
    "                best_loss = val_loss\n",
    "\n",
    "            if epoch_counter%20 == 0:\n",
    "                print(f\"Epoch {epoch_counter}/{EPOCHS}, Train Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}\")\n",
    "\n",
    "        print(f\"Best validation loss : {best_loss}\")\n",
    "        best_losses.append(best_loss)\n",
    "        \n",
    "        ### Visualize train-validation loss\n",
    "\n",
    "#         plt.plot(range(len(tr_losses_fp)),tr_losses_fp,color='blue',label='train_loss')\n",
    "#         plt.plot(range(len(val_losses_fp)),val_losses_fp,color='red',label='val_loss')\n",
    "#         plt.legend()\n",
    "#         # plt.show()\n",
    "#         plt.savefig(f'../../plots/train_val_loss_2/{filename}.png')\n",
    "#         plt.clf()\n",
    "        globals()[f'train_losses_{trial}']=tr_losses_fp\n",
    "        globals()[f'val_losses_{trial}']=val_losses_fp\n",
    "\n",
    "        ### Prediction\n",
    "\n",
    "        # evaluation\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            X_test_tensor = torch.tensor(X_test.astype(np.float32), dtype = torch.float32,device=device)\n",
    "            pred = model(X_test_tensor)\n",
    "\n",
    "        pred = pred.to('cpu').detach().numpy()\n",
    "        # print(pred)\n",
    "\n",
    "        if TARGET == \"1d_ROC\" :\n",
    "            endPrice = data['Close'].iloc[-1]\n",
    "            pred_close = []\n",
    "\n",
    "            for i in pred[0] :\n",
    "                endPrice = endPrice + endPrice*0.01*i\n",
    "                pred_close.append(endPrice)\n",
    "\n",
    "            pred = np.array(pred_close).reshape(1, PRED_SIZE)\n",
    "        #     pred\n",
    "        else :\n",
    "            pass\n",
    "\n",
    "        globals()[f'pred_{trial}'] = pred\n",
    "        \n",
    "        # pred_length = len(np.reshape(pred, (-1)))\n",
    "        # pred_indices = list(range(pred_length))\n",
    "        # plt.plot(pred_indices, np.reshape(pred, (-1)), color='red', alpha=0.6, label='Prediction')\n",
    "        # plt.legend()\n",
    "\n",
    "        # plt.show()\n",
    "\n",
    "        ### Evaluation\n",
    "\n",
    "        # Label\n",
    "#         testfile = '../../data/test.csv'\n",
    "#         test_df = pd.read_csv(testfile)\n",
    "#         label = pd.read_csv(testfile) # Í∞ÅÏûê test.csv ÌååÏùº Í≤ΩÎ°ú\n",
    "        label = test_df.copy()\n",
    "        label = np.array(label.head(PRED_SIZE)[\"Close Price\"])\n",
    "\n",
    "        # Prediction\n",
    "        pred = np.array(pred).reshape(PRED_SIZE)\n",
    "\n",
    "        # ÎÇ†Ïßú Îç∞Ïù¥ÌÑ∞\n",
    "        period = test_df[\"Date\"].copy()\n",
    "        period = [d for d in period.head(PRED_SIZE)]\n",
    "\n",
    "        # Ïò§Ï∞®Ïú® Í≥ÑÏÇ∞\n",
    "        error_rate = np.abs((label - pred) / label) * 100\n",
    "\n",
    "        # ÏãúÍ∞ÅÌôî\n",
    "#         plt.figure(figsize=(12, 6))\n",
    "#         plt.plot(period, label, marker='o', color='blue', label='Actual Close Price')\n",
    "#         plt.plot(period, pred, marker='x', color='red', linestyle='--', label='Predicted Close Price')\n",
    "\n",
    "#         # Ïò§Ï∞®Ïú®ÏùÑ Í∞Å Ìè¨Ïù∏Ìä∏Ïóê ÌÖçÏä§Ìä∏Î°ú ÌëúÏãú\n",
    "#         for date, lbl, prd, err in zip(period, label, pred, error_rate):\n",
    "#             plt.text(date, prd, f'{err:.2f}%', color='black', ha='right', va='bottom')\n",
    "\n",
    "#         plt.xticks(rotation = 45)  # ÎÇ†Ïßú Î†àÏù¥Î∏î ÌöåÏ†Ñ\n",
    "#         plt.xlabel('Date')\n",
    "#         plt.ylabel('Close Price')\n",
    "#         plt.title(f'model = LSTM, data = {dataname}, target = {TARGET}, seq_size = {SEQ_SIZE}, pred_size = {PRED_SIZE}, batch_size = {BATCH_SIZE}, model_size = {HIDDEN_SIZE}')\n",
    "#         plt.legend()\n",
    "#         plt.tight_layout()  # Î†àÏù¥ÏïÑÏõÉ Ï°∞Ï†ï\n",
    "#         # plt.show()\n",
    "#         plt.savefig(f'../../plots/results_2/{filename}.png')\n",
    "#         plt.clf()\n",
    "\n",
    "        # Ïò§Ï∞®Ïú®ÏùÑ Ï∂úÎ†•\n",
    "        error_rate_dict = dict(zip(period, error_rate))\n",
    "        # error_rate_dict\n",
    "\n",
    "        # ÌèâÍ∑† Ïò§Ï∞®Ïú® Í≥ÑÏÇ∞\n",
    "        average_error_rate = np.mean(error_rate)\n",
    "        \n",
    "        print(f\"Average Error Rate: {average_error_rate:.2f}%\")\n",
    "        mean_error_ratios.append(average_error_rate)\n",
    "        \n",
    "        ### Save results to a DataFrame\n",
    "#         temp_df = pd.DataFrame({\n",
    "#             'data':[dataname],\n",
    "#             'target':[TARGET],\n",
    "#             'seq_size':[SEQ_SIZE],\n",
    "#             'pred_size':[PRED_SIZE],\n",
    "#             'batch_size':[BATCH_SIZE],\n",
    "#             'hidden_size':[HIDDEN_SIZE],\n",
    "#             'best_val_loss':[best_loss],\n",
    "#             'mean_error_ratio':[average_error_rate]\n",
    "#         })\n",
    "\n",
    "#         result_df = pd.concat([result_df,temp_df],axis=0)\n",
    "print('üî∏ENDüî∏')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82d4fcc9",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-02-22T06:02:37.939Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "preds = []\n",
    "for t in list(range(1,11)):\n",
    "    preds.append(globals()[f'pred_{t}'].tolist())\n",
    "preds = np.reshape(np.array(preds),(10,10))\n",
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7068db8",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-02-22T06:02:37.940Z"
    }
   },
   "outputs": [],
   "source": [
    "preds_df = pd.DataFrame(preds,columns=['model_1','model_2',\n",
    "                           'model_3','model_4',\n",
    "                           'model_5','model_6',\n",
    "                           'model_7','model_8',\n",
    "                           'model_9','model_10'])\n",
    "preds_df['Mean']=preds_df.mean(axis=1)\n",
    "preds_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c21023e",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-02-22T06:02:37.942Z"
    }
   },
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame([best_losses,mean_error_ratios], columns=[\n",
    "                            'model_1','model_2',\n",
    "                           'model_3','model_4',\n",
    "                           'model_5','model_6',\n",
    "                           'model_7','model_8',\n",
    "                           'model_9','model_10'])\n",
    "print('Mean Val_Loss :',results_df.iloc[0].mean())\n",
    "print('Mean Error Ratio :',results_df.iloc[1].mean())\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81a853fe",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-02-22T06:02:37.943Z"
    }
   },
   "outputs": [],
   "source": [
    "# ÏãúÍ∞ÅÌôî\n",
    "label = test_df.copy()\n",
    "label = np.array(label.head(PRED_SIZE)[\"Close Price\"])\n",
    "\n",
    "# Prediction\n",
    "pred_ = preds_df['Mean'].to_numpy().reshape(PRED_SIZE)\n",
    "\n",
    "# ÎÇ†Ïßú Îç∞Ïù¥ÌÑ∞\n",
    "period = test_df[\"Date\"].copy()\n",
    "period = [d for d in period.head(PRED_SIZE)]\n",
    "\n",
    "# Ïò§Ï∞®Ïú® Í≥ÑÏÇ∞\n",
    "error_rate = np.abs((label - pred_) / label) * 100\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(period, label, marker='o', color='blue', label='Actual Close Price')\n",
    "plt.plot(period, pred_, marker='x', color='red', linestyle='--', label='Predicted Close Price')\n",
    "\n",
    "# Ïò§Ï∞®Ïú®ÏùÑ Í∞Å Ìè¨Ïù∏Ìä∏Ïóê ÌÖçÏä§Ìä∏Î°ú ÌëúÏãú\n",
    "for date, lbl, prd, err in zip(period, label, pred_, error_rate):\n",
    "    plt.text(date, prd, f'{err:.2f}%', color='black', ha='right', va='bottom')\n",
    "\n",
    "plt.xticks(rotation = 45)  # ÎÇ†Ïßú Î†àÏù¥Î∏î ÌöåÏ†Ñ\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Close Price')\n",
    "plt.title(f'model = {MODEL}, data = {dataname}, target = {TARGET}, seq_size = {SEQ_SIZE}, pred_size = {PRED_SIZE}, batch_size = {BATCH_SIZE}, model_size = {HIDDEN_SIZE}')\n",
    "plt.legend()\n",
    "plt.tight_layout()  # Î†àÏù¥ÏïÑÏõÉ Ï°∞Ï†ï\n",
    "plt.show()\n",
    "plt.savefig(f'../../plots/best_plots/50trials_{filename}.png') #### Î≥∏Ïù∏ ÌååÏùº Í≤ΩÎ°ú\n",
    "plt.clf()\n",
    "\n",
    "# Ïò§Ï∞®Ïú®ÏùÑ Ï∂úÎ†•\n",
    "error_rate_dict = dict(zip(period, error_rate))\n",
    "# error_rate_dict\n",
    "\n",
    "# ÌèâÍ∑† Ïò§Ï∞®Ïú® Í≥ÑÏÇ∞\n",
    "average_error_rate = np.mean(error_rate)\n",
    "\n",
    "print(f\"Average Error Rate: {average_error_rate:.2f}%\")\n",
    "mean_error_ratios.append(average_error_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16ae98fa",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-02-22T06:02:37.944Z"
    }
   },
   "outputs": [],
   "source": [
    "### Visualize train-validation loss\n",
    "\n",
    "for t in list(range(1,11)):\n",
    "    plt.plot(range(len(globals()[f'train_losses_{t}'])),\n",
    "             globals()[f'train_losses_{t}'],\n",
    "             color='blue',alpha=0.6)\n",
    "    plt.plot(range(len(globals()[f'val_losses_{t}'])),\n",
    "             globals()[f'val_losses_{t}'],\n",
    "             color='red',alpha=0.6)\n",
    "plt.legend(['train_loss','val_loss'])\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "plt.savefig(f'../../plots/best_plots/50trials_train_val_loss_{filename}.png')\n",
    "plt.clf()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
